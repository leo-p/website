---
title: "[Paper] BERT"
date: 2019-11-02T08:28:37+09:00
draft: true
categories:
  - paper-review
tags:
  - tech
  - computer-vision
  - deep-learning
---

- **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**<br/>
⭐️️️️️️️⭐️️️️️️️️⭐️️️️️⭐️⭐️ | [arXiv](https://arxiv.org/abs/1810.04805), [GitHub](https://github.com/google-research/bert)

BERT is a great breakthrough for NLP. It's been compared to the ImageNet/pre-training breakthrough for computer vision, effectively allowing fine-tuning on several task.

I won't try to explain everything in this post, mostly because there are really already great resources:

- [The Illustrated Transformer by Jay Alammar](https://jalammar.github.io/illustrated-transformer/): goes over the fundamental building block of BERT.

- [The Illustrated BERT, ELMo, and co. by Jay Alammar](https://jalammar.github.io/illustrated-bert/): deep-dives into the actual BERT model.

At a high-level, BERT is trained to reconstruct sentences with random element masked. Given the phrase `I love [MASK] and dogs` it should output the original sentence `I love cats and dogs`.

This lead it to generate a great understanding of the world and which is then reused for finetuning on different tasks.

Another key element of BERT is its attention mechanism, which takes into account the whole sentence and attributes different weights to other words in the sentence.

BERT comes in two shapes, a Large and a Small model.

**Summary:**

- Introduce two new BERT models.
- Reach state-of-the-art on several tasks.
- Transformer and attention based.
